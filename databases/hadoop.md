### Основные понятия
**Hadoop** - проект фонда Apache Software Foundation, свободно распространяемый набор утилит, библиотек и фреймворк для разработки и выполнения распределённых программ, работающих на кластерах из сотен и тысяч узлов.  Разработан на Java в рамках вычислительной парадигмы MapReduce. Основные модули: Hadoop Common, HDFS, YARN и Hadoop MapReduce (платформа программирования и выполнения распределённых MapReduce-вычислений). 
  
**HDFS** - файловая система, предназначенная для хранения файлов больших размеров, поблочно распределённых между узлами вычислительного кластера. Все блоки в HDFS (кроме последнего блока файла) имеют одинаковый размер, и каждый блок может быть размещён на нескольких узлах, размер блока и коэффициент репликации определяются в настройках на уровне файла. Файлы в HDFS могут быть записаны лишь однажды (модификация не поддерживается), а запись в файл в одно время может вести только один процесс. Организация файлов в пространстве имён — традиционная иерархическая: есть корневой каталог, поддерживается вложение каталогов, в одном каталоге могут располагаться и файлы, и другие каталоги. Hadoop-кластер состоит из нод трех типов: NameNode, Secondary NameNode, Datanode.

**Namenode** — как правило, одна нода на кластер. Хранит в себе все метаданные системы — непосредственно маппинг между файлами и блоками. Если нода одна, то она же и является Single Point of Failure. Эта проблема решена во второй версии Hadoop с помощью Namenode Federation.

**Secondary NameNode** — 1 нода на кластер. Принято говорить, что «Secondary NameNode» — это одно из самых неудачных названий за всю историю программ. Действительно, Secondary NameNode не является репликой NameNode. Состояние файловой системы хранится непосредственно в файле fsimage и в лог файле edits, содержащим последние изменения файловой системы (похоже на лог транзакций в мире РСУБД). Работа Secondary NameNode заключается в периодическом мерже fsimage и edits. Secondary NameNode необходима для быстрого ручного восстанавления NameNode в случае выхода NameNode из строя. В реальном кластере NameNode и Secondary NameNode — отдельные сервера, требовательные к памяти и к жесткому диску.  

**DataNode** — таких нод в кластере очень много. Они хранят непосредственно блоки файлов. Нода регулярно отправляет NameNode свой статус (показывает, что еще жива) и ежечасно — репорт, информацию обо всех хранимых на этой ноде блоках. Это необходимо для поддержания нужного уровня репликации.
  
**MapReduce** - это фреймворк для вычисления некоторых наборов распределенных задач с использованием большого количества нод, образующих кластер.  

**Hadoop MapReduce** — программный каркас для программирования распределённых вычислений в рамках парадигмы MapReduce. Разработчику приложения для Hadoop MapReduce необходимо реализовать обработчики mapper и reducer:  
1 - map (mapper) - на каждом вычислительном узле кластера обеспечит преобразование исходных пар key-value в промежуточный набор пар key-value.   
2 - combine (mapper, optional) = reduce на локальном уровне, применяется для уменьшения промежуточного объема данных  
3 - shuffle, sort (reducer, optional) - перемешивания и сортировка для доставки пар key-value с одинаковым ключом на один reducer  
4 - reduce (reducer, optional) - преобразует список к единственному атомарному значению при помощи заданной функции, которой на каждой итерации передаются новый элемент списка и промежуточный результат.  
  
**Hadoop Common** - в Hadoop Common входят библиотеки управления файловыми системами, поддерживаемыми Hadoop, и сценарии создания необходимой инфраструктуры и управления распределённой обработкой, для удобства выполнения которых создан специализированный упрощённый интерпретатор командной строки (FS shell, filesystem shell), запускаемый из оболочки операционной системы. Бо́льшая часть команд интерпретатора реализована по аналогии с соответствующими командами Unix, есть команды, специфические для Hadoop.  
  
### YARN
**YARN** - модуль, появившийся с версией hadoop 2.0 (2013), отвечающий за управление ресурсами кластеров и планирование заданий. Если в предыдущих выпусках эта функция была интегрирована в модуль MapReduce (MR1), где была реализована единым компонентом (JobTracker), то в YARN функционирует логически самостоятельный демон — планировщик ресурсов (ResourceManager), абстрагирующий все вычислительные ресурсы кластера и управляющий их предоставлением приложениям распределённой обработки. Работать под управлением YARN могут как MapReduce-программы, так и любые другие распределённые приложения, поддерживающие соответствующие программные интерфейсы. YARN обеспечивает возможность параллельного выполнения нескольких различных задач в рамках кластера и их изоляцию. Разработчику распределённого приложения необходимо реализовать специальный класс управления приложением (ApplicationMaster), который отвечает за координацию заданий в рамках тех ресурсов, которые предоставит планировщик ресурсов; планировщик ресурсов же отвечает за создание экземпляров класса управления приложением и взаимодействия с ним через соответствующий сетевой протокол. YARN может быть рассмотрен как кластерная операционная система в том смысле, что выступает интерфейсом между аппаратными ресурсами кластера и широким классом приложений, использующих его мощности для выполнения вычислительной обработки.  

MR1 разделил свои функции: YARN (управление ресурсами), MR2 (логика MR вычислений, реализовано через MRAM)  
  
Недостатки MR1 по сравнению с YARN: ресурсы разделены на слоты (ядро на слот), которые резервируются под все маперы и редьюсеры задания (т.е. map и reduce работают последовательно, но слоты резервируются под все задание на все время работы задания); нет возможности разделять ресурсы с не MR задачами; ограничения в масштабируемости. YARN оперирует ресурсами (память и процессор) и не разделяет мапперы и редьюсеры.

Компоненты YARN:
1. Resource Manager (RM) - аналог JobTracker из MR1
    * запущен на отдельном кластере
    * управляет глобальным распределением ресурсов
    * разрешает конфликты между конкурирующими приложениями (очереди, планировщик)
    * управляет нодами, контейнерами
    * взаимодействует с AM
    * не знает ничего о типе запускаемого приложения
    * производит выделение памяти в установленных границах с определенной кратностью
    * перезапускает AM при его сбое, новый AM продолжает работать с того места, где умер предыдущий AM
    * убирает NM из списка активных, если от NM нет сигналов
    * есть stand by RM
2. Node Manager (NM) - аналог Task Traker из MR1
    * запущен на всех нодах кластера
    * взаимодействует с RM для выделения ресурсов на ноде
    * взаимодействует с RM - мониторинг работы + информация о ресурсах ноды + состояние контейнеров
    * управляет процессами в контейнерах - утилизация ресурсов (отчтреливает процессы при превышении лимитов выделенных ресурсов), логи
    * включает в себя Shuffle Service - производит перераспределение перед reduce фазой
3. Containers
    * создается по запросу RM через NM нв нодах
    * резервирует ресурсы (память, CPU)
    * приложение запускается на одном или нескольких контейнерах
4. Application Master (AM) - часть функционала JobTracker из MR1
    * один для задачи, знает информацию о задаче - ресурсы, маперы, редьюсеры и тд
    * создается занаво для каждой задачи
    * создается по запросу RM через NM на какой-либо ноде кластера
    * запускается в контейнере (также созданному по запросу RM)
    * зависит от типа задачи (для MR задачи - MRAppMaster)
    * запрашивает другие контейнеры для запуска приложений - сообщает требования по ресурсам и окружению и ноду, где нужно запустить контейнеры
    * после получения инфы о создании контейнеров запускает на них приложение (воркеры для MR задачи)
    * MRAppMaster выполняет два предыдущий пункта последовательно для map и reduce фазы, после завершения map фазы ресурсы на ноде высвобождаются 
    * мониторит работу приложения (мапперы и редьюсеры для MR задач) на нодах и сообщает RM статус приложения
    * обращается в Shuffle Service
    * перезапускает задачу при сбое контейнера (есть макс чисто попыток)
    * заносит NM в чс, если он на нем падают контейнеры

Uber задача - задача в одной JVM, используется, когда запуск всех компонент съедает кол-во ресурсов, сравнимое с основной задачей.
  
### Создание таблиц/внешних таблиц/вьюх в Hive  
  
### Мониторинг Hadoop  
  
### Парсинг Json (Json-SerDe, json_tuple, get_json_object)  
### Партиционироване / Partitions recover (msck repair table)  
### Сжатие таблиц  
  
### Работа с консольной утилитой hdfs  
  
Avro  
ORC  
Parquet  

___
[Hadoop — Википедия](https://ru.wikipedia.org/wiki/Hadoop)  
[Поговорим за Hadoop / Хабр](https://habr.com/ru/companies/dataart/articles/234993/)  
[Hadoop: что, где и зачем / Хабр](https://habr.com/ru/articles/240405/)  
[Что такое MapReduce: принципы работы главной технологии Big Data](https://bigdataschool.ru/wiki/mapreduce)  
[]()  
[]()  
