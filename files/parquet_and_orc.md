## Parquet
Parquet — это бинарный, колоночно-ориентированный формат хранения больших данных, изначально созданный для экосистемы Hadoop, позволяющий использовать преимущества сжатого и эффективного колоночно-ориентированного представления информации. Паркет позволяет задавать схемы сжатия на уровне столбцов и добавлять новые кодировки по мере их появления.  
  
Parquet использует архитектуру, основанную на “уровнях определения” (definition levels) и “уровнях повторения” (repetition levels), что позволяет довольно эффективно кодировать данные, а информация о схеме выносится в отдельные метаданные.
При этом оптимально хранятся и пустые значения.
  
<img src="https://github.com/oldos-orwell/data_notes/blob/main/images/parquet.png"/>  
  
* File Header - содержит магическое число PAR1, которое позволяет прочитать и определить, что это Parquet-файл для его дальнейшей оптимизации.
* Row-group - группы строк - разбиение одного конкретного файла на множество маленьких блоков для того, чтобы в случае необходимости не приходилось вычитывать весь огромный файл. Позволяет параллельно работать с данными на уровне Map-Reduce
* Column Chunk - разбиение по колонкам - каждую колонку можно прочитать отдельно. Оптимизирует работу с диском. Если представить данные как таблицу, то они записываются не построчно, а по колонкам.  
  
| A  | C  | C  |  
|----|----|----|  
| a1 | b1 | c1 |  
| a2 | b2 | c2 |  
| a3 | b3 | c3 |

Строковое хранение: [a1, b1, c1, a2, b2, c2, a3, b3, c3]  
Колоночное хранение: [a1, a2, a3, b1, b2, b3. c1, c2, c3]
 
* Page - физическое представление данных, набор значений для каждого столбца в файле. Page имеет собственное описание схемы данных, которое указывает на тип данных, размер конкретной страницы и способ кодирования. Также как в Row-group, там возможны ссылки на словарь для того, чтобы декодировать данные. Позволяет распределять работу по кодированию и сжатию - за счёт разделения достигается довольно эффективное и быстрое кодирование, на данном уровне производится сжатие (если оно настроено).
* File Footer - содержит:  
  * Метаданные (определение схемы, информация о группах строк, метадано стоные лбцах);
  * Дополнительные данные, например, местоположение словаря для столбца;
  * Статистику по столбцам.
  
Parquet явно отделяет метаданные от данных, что позволяет разбивать столбцы на несколько файлов, а также иметь один файл метаданных, ссылающийся на несколько файлов паркета. Метаданные записываются после значащих данных, чтобы обеспечить однопроходную запись. Таким образом, сначала прочитаются метаданные файла, чтобы найти все нужные фрагменты столбцов, которые дальше будут прочтены последовательно.  
   
Достоинства хранения данных в Parquet:
* Несмотря на то, что они и созданы для hdfs, данные могут храниться и в других файловых системах, таких как GlusterFs или поверх NFS
* По сути это просто файлы, а значит с ними легко работать, перемещать, бэкапить и реплицировать.
* Колончатый вид позволяет значительно ускорить работу аналитика, если ему не нужны все колонки сразу.
* Нативная поддержка в Spark из коробки обеспечивает возможность просто взять и сохранить файл в любимое хранилище.
* Эффективное хранение с точки зрения занимаемого места.

## ORC
ORC (Optimized Row Columnar) - колоночно-ориентированный (столбцовый) формат хранения Big Data в экосистеме Apache Hadoop. Он совместим с большинством сред обработки больших данных в среде Apache Hadoop и похож на другие колоночные форматы файлов: RCFile и Parquet.  
  
<img src="https://github.com/oldos-orwell/data_notes/blob/main/images/orc2.png"/>  


___
[Форматы файлов в больших данных: краткий ликбез / Хабр](https://habr.com/ru/companies/vk/articles/504952/)  
[Что такое формат данных Apache Parquet для Big Data файлов](https://bigdataschool.ru/wiki/parquet)  
[Как использовать Parquet и не поскользнуться / Хабр](https://habr.com/ru/companies/wrike/articles/279797/)  
[Форматы ORC и Parquet на базе HDFS / Хабр](https://habr.com/ru/companies/oleg-bunin/articles/761780/)  
